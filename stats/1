
import os
import sys
import pystac
import rioxarray
import xarray as xr
from glob import glob
# from osgeo import gdal
# import geopandas as gpd
# from shapely.geometry import mapping
import pandas as pd
import numpy as np

# Set matplotlib backend
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend for headless environments
import matplotlib.pyplot as plt

CATALOG_URL = "https://s3.waw3-2.cloudferro.com/swift/v1/wpl-stac/stac/catalog.json"

def read_stac_data(site, variable):
    """
    Reads ...
    """
    root: pystac.Catalog = pystac.read_file(CATALOG_URL) 

    # Get the sub-catalog for the site
    catalog: pystac.Catalog = root.get_child(site)

    # Get the collection for the corresponding variable
    collection: pystac.Collection = catalog.get_child(variable)

    from IPython import embed ; ipshell = embed()

    # This dataset is chunked for spatial reads
    asset = collection.assets[f"{variable}.xy.zarr"]
    
    ds = xr.open_dataset(
        asset.href,
        **asset.ext.xarray.open_kwargs,  # type: ignore
    )

    # If variable is water-level transform the 95% confidence interval
    # into the standard error. Since A 95% confidence interval corresponds
    # to approximately 1.96 standard deviations, let's get the standard
    # error dividing by 1.96...
    if variable == 'water-level':
        ds['confidence_interval'] /= 1.96

        # TODO: REMOVE this, I'm getting the variable name from a hardcoded list!
        # Sorry Sam, could we make consistent the STAC variable name and 
        # the Dataset variable name? or even better get the asset name from the metadata
        ds = ds.rename({'water_level' : 'water-level'})

    if variable == 'annual-surface-velocity':
        ds = ds.rename({'annual-surface-velocity' : 'surface-velocity'})

    return ds

def read_data_and_uncertainty(data_path, uncertainty_path):
    """
    Reads WorldPeatland data and associated uncertainty GeoTIFFs as xarray.DataArray.
    """
    lai = rioxarray.open_rasterio(data_path)
    uncertainty = rioxarray.open_rasterio(uncertainty_path)

    # Read per-band metadata for time
    d = gdal.Open(data_path)
 
    bands = d.RasterCount
    band_times = []

    for i in range(1, bands + 1):
        tags = d.GetRasterBand(i).GetMetadata()
        # Try several possible keys for date
        date_str = tags.get("RANGEBEGINNINGDATE") or tags.get("DATE") or tags.get("time") or None
        if date_str is not None:
            # Accept both YYYY-MM-DD and YYYY-MM-DDTHH:MM:SS formats
            date_str = date_str.split("T")[0]
            band_times.append(np.datetime64(date_str))
        else:
            band_times.append(np.datetime64('NaT'))

    # Rename dimensions and assign coordinates
    lai = lai.rename({"band": "time", "y": "latitude", "x": "longitude"})
    uncertainty = uncertainty.rename({"band": "time", "y": "latitude", "x": "longitude"})
    lai = lai.assign_coords(time=("time", band_times))
    uncertainty = uncertainty.assign_coords(time=("time", band_times))

    return lai, uncertainty

def get_pixel_indices_within_classification(classification_path):
    """
    Returns the indices (y, x) of pixels where the classification
    is labeled as peatland, value is 1.
    """
    # Read the classification raster
    classification = rioxarray.open_rasterio(classification_path)

    # Find indices where classification is not nan (i.e., inside the classification)
    indices = np.argwhere(classification.isel(band=0).data==1)

    return indices

def get_pixel_indices_within_geometry(dataarray, shapefile_path):
    """
    Returns the indices (band, y, x) of pixels in dataarray that overlap with 
    the geometry in shapefile_path.
    """
    # Read the shapefile
    gdf = gpd.read_file(shapefile_path)
    # Reproject geometry to match raster CRS
    gdf = gdf.to_crs(dataarray.rio.crs)

    # Rasterize the geometry to the shape of the dataarray
    mask = dataarray.rio.clip(gdf.geometry.apply(mapping), gdf.crs,
            all_touched=True, drop=False, invert=False)

    # Find indices where mask is not nan (i.e., inside the geometry)
    indices = np.argwhere(~np.isnan(mask.values))

    return indices

def get_weighted_mean_and_uncertainties(data, variable_name, uncertainty_name,
                                        spatial_ratio, indices):
    """
    Calculate weighted mean and variance for the data at the specified indices using uncertainty as weights.
    Weights are computed as the inverse of the square of uncertainty values.
    Also computes an uncertainty ratio based on weight distribution.
    Returns a DataFrame with the weighted mean, and uncertainty for each time step.
    """ 
    if len(indices) == 0:
        return None, None

    weighted_means = []
    uncertainties = []

    # for i in range(data[variable_name].shape[0]):
    for i in range(100,110):

        print(f"Processing time step {i+1}/{data[variable_name].shape[0]}...")

        # Select pixels where classification is 1
        data_vals = data[variable_name].values[i, indices[:, 0], indices[:, 1]]

        # TODO: find a better solution for this
        if variable_name == 'water-level':
            unc_vals = data[uncertainty_name].values[indices[:, 0], indices[:, 1]]
        else:
            unc_vals = data[uncertainty_name].values[i, indices[:, 0], indices[:, 1]]

        # Compute weights
        weights = 1.0 / (unc_vals ** 2)

        # Avoid division by zero or nan
        # valid = np.isfinite(data_vals) & np.isfinite(weights) & (weights > 0)
        valid = np.isfinite(data_vals) & np.isfinite(weights) & (weights > 0)
        data_vals = data_vals[valid]
        weights = weights[valid]

        if weights.size == 0:
            weighted_mean = np.nan
            uncertainty = np.nan
        else:
            # Calculate weighted mean
            weighted_mean = np.sum(data_vals * weights) / np.sum(weights)

            # Calculate uncertainty
            unique_weights, counts = np.unique(weights, return_counts=True)
           
            # Numerator: for each unique weight, spatial_ratio^2 / count occurrences, multiply by weight, sum
            numerator = np.sum((spatial_ratio**2 / counts) * unique_weights)

            # Denominator: for each unique weight, count occurrences, multiply by weight, sum
            denominator = np.sum(counts * unique_weights)
            
            # Compute the ratio
            uncertainty = numerator / denominator if denominator != 0 else np.nan

            print(f"Weighted mean: {weighted_mean}, Uncertainty: {uncertainty}")

        weighted_means.append(weighted_mean)
        uncertainties.append(uncertainty)

    # Create a DataFrame to stores the weighted means, variances, and uncertainty ratios
    df = pd.DataFrame({"weighted_mean": weighted_means,
                       "uncertainty": uncertainties},
                       index=data['time'].values[100:110])
    
    return df

def create_plot(weighted_mean, weighted_variance, variable):
    """
    Create a plot of the zonal stats time series
    """
    # Create figure
    fig, ax = plt.subplots(figsize=(10, 5))

    # Plot weighted mean time series
    ax.plot(weighted_mean.index,
            weighted_mean,
            label=f"{variable} - weighted mean", color="C0")

    # Compute standard deviation from variance
    std = np.sqrt(weighted_variance.values)

    # Fill between mean Â± std
    ax.fill_between(
        weighted_mean.index,
        weighted_mean - std,
        weighted_mean + std,
        color="C0",
        alpha=0.3,
       label=f"{variable} - weighted std dev"
    )

    ax.set_xlabel("Time")
    ax.set_ylabel(variable)
    ax.set_title(f"{variable}")
    ax.legend()
    fig.autofmt_xdate()
    plt.grid()
    plt.tight_layout()

    plt.savefig(f"/tmp/{variable}_weighted_mean_and_uncert.png", dpi=150)

def extract_zonal_stats(variable_name, uncertainty, spatial_ratio,
                        site, classification_fname, plot=False):
    """
    Extract zonal stats using associated uncertainties
        The stats then will be linearly interpolated to create
        synthetic daily data
    """
    # Read data and associated uncertainty
    data = read_stac_data(variable=variable_name, site=site)

    # Get the indices where the classification is 1
    indices = get_pixel_indices_within_classification(classification_fname)

    # Compute stats
    weighted_stats = get_weighted_mean_and_uncertainties(data=data,
                         variable_name=variable_name,
                         uncertainty_name=uncertainty_name,
                         spatial_ratio=spatial_ratio, indices=indices)

    # Linear interpolation to create synthetic daily data
    weighted_mean = weighted_stats['weighted_mean'].resample('D').interpolate('linear')
    uncertainty = weighted_stats['uncertainty'].resample('D').interpolate('linear')

    if plot == True:
        create_plot(weighted_mean, uncertainty, variable_name)

    return weighted_mean, uncertainty


if __name__ == "__main__":

    if len(sys.argv) != 3:

        # Check inputs
        print((f"Usage: python .zonal_stats_with_uncert.py"
               f"<site_root_data_dir> <peatland_extent_path>"))
    else:
        # Site name e.g. degero
        site = sys.argv[1]

        # Full path of the peatland classification GeoTiff
        # e.g. /wp_data/sites/Degero/WhatSARPeat/WhatSARPeat2024_Degero.tif
        classification_fname = sys.argv[2]

        # Variable name, uncertainty name, spatial ratio
        # Spatial ratio is nominal spatial res /  20
        # since 20m is the spatial res of the stored products
        variables = [['lai', 'lai_std_dev', 25],
                     ['fpar', 'fpar_std_dev', 25],
                     ['albedo', 'albedo_std_dev', 25],
                     ['evi', 'evi_std', 50],
                     ['lst_day', 'lst_day_std_dev]', 50],
                     ['lst_night', 'lst_night_std_dev', 50],
                     ['lst_diurnal_range', 'lst_diurnal_range_std_dev', 50],
                     ['velocity', 'velocity_std_dev', 1],
                     ['water-level', 'confidence_interval', 5]]

        variables = [['annual-surface-velocity', 'velocity_std_dev', 1]]

        data = pd.DataFrame()
        uncertainty = pd.DataFrame()

        for variable_name, uncertainty_name, spatial_ratio in variables:
            print(f"Processing {variable_name}...")
            w_mu, w_unc = extract_zonal_stats(variable_name, uncertainty_name, 
                                              spatial_ratio, site,
                                              classification_fname, plot=True)
            print(w_mu, w_unc)

            data[variable_name] = w_mu
            uncertainty[variable_name] = w_unc

        filename = f"time_series_{site}.h5"
        data.to_hdf(filename, key="data")
        uncertainty.to_hdf(filename, key="uncertainty")
